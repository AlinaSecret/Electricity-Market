{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1740749088019,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"},"user_tz":-120},"id":"qPy1npqvaIMe"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"kChfkhkDWFvs"},"source":["#Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4635,"status":"ok","timestamp":1740749092655,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"},"user_tz":-120},"id":"LcR0xq9ZWI9g","outputId":"57e8f195-b2ba-4069-edfa-11328a1ee6cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","The following additional packages will be installed:\n","  freeglut3 libegl-dev libfontenc1 libgl-dev libgl1-mesa-dev libgles-dev libgles1 libglu1-mesa\n","  libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev libice-dev libopengl-dev libsm-dev\n","  libxfont2 libxkbfile1 libxt-dev x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","Suggested packages:\n","  libice-doc libsm-doc libxt-doc\n","The following NEW packages will be installed:\n","  freeglut3 freeglut3-dev libegl-dev libfontenc1 libgl-dev libgl1-mesa-dev libgles-dev libgles1\n","  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev libice-dev libopengl-dev\n","  libsm-dev libxfont2 libxkbfile1 libxt-dev x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common xvfb\n","0 upgraded, 25 newly installed, 0 to remove and 49 not upgraded.\n","Need to get 893 kB/9,076 kB of archives.\n","After this operation, 18.7 MB of additional disk space will be used.\n","Ign:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.12\n","Ign:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.12\n","Err:1 http://security.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.12\n","  404  Not Found [IP: 185.125.190.83 80]\n","Err:2 http://security.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.12\n","  404  Not Found [IP: 185.125.190.83 80]\n","E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/x/xorg-server/xserver-common_21.1.4-2ubuntu1.7%7e22.04.12_all.deb  404  Not Found [IP: 185.125.190.83 80]\n","E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/universe/x/xorg-server/xvfb_21.1.4-2ubuntu1.7%7e22.04.12_amd64.deb  404  Not Found [IP: 185.125.190.83 80]\n","E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n","Requirement already satisfied: stable-baselines3>=2.0.0a4 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.6.0a1)\n","Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.0.0)\n","Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.26.4)\n","Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.5.1+cu121)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.1.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.8.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (4.10.0.84)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.6.1)\n","Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.17.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (5.9.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (4.67.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (13.9.4)\n","Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (0.10.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (11.0.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py>=0.9.0->stable-baselines3[extra]>=2.0.0a4) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.0.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.68.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (24.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (4.25.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (75.1.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.1.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2024.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a4) (0.1.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.0.2)\n"]}],"source":["!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n","!pip install \"stable-baselines3[extra]>=2.0.0a4\""]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4178,"status":"ok","timestamp":1740749096863,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"},"user_tz":-120},"id":"gVd8rL1W6CJf"},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","from gymnasium import spaces\n","from stable_baselines3.common.env_checker import check_env\n"]},{"cell_type":"markdown","metadata":{"id":"XNRk05DSVz4H"},"source":["#Environment"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1740749096873,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"},"user_tz":-120},"id":"TFNOOYSFgg3X","outputId":"2f13b91b-ff2c-4c92-e5a7-3153513c99cf"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["def seasonal_duck_curve(hour, season, noise_scale=0.03):\n","    \"\"\" Computes electricity demand based on seasonal duck curve \"\"\"\n","    A = 400\n","    seasonal_params = {\n","        1: (100, 7, 80, 12, 120, 14, 250, 18, 30, 4),  # Summer\n","        2: (180, 6, 40, 12, 80, 14, 220, 17, 60, 4),  # Winter\n","        3: (130, 7, 90, 12, 100, 14, 180, 18, 40, 4)  # Spring/Autumn\n","    }\n","    B, t_morning, C, t_dip, D, mu_dip, E, t_evening, F, t_early = seasonal_params[season]\n","    demand = A + B / (1 + np.exp(-1 * (hour - t_morning))) - C / (1 + np.exp(-1 * (hour - t_dip))) - D * np.exp(-((hour - mu_dip)**2) / 4) + E / (1 + np.exp(-1 * (hour - t_evening)))\n","    return demand * random.uniform(0.9, 1.1)\n","\n","def electricity_price_function(hour, season, demand, noise_scale=0.03):\n","    \"\"\" Computes electricity price based on seasonal demand \"\"\"\n","    season_params = {\n","        1: (30, 15, 10),  # Summer\n","        2: (28, 14, 9),   # Winter\n","        3: (25, 12, 8)    # Spring/Autumn\n","    }\n","    A_q, B_q, C_q = season_params[season]\n","    base_price = A_q + B_q * np.cos(2 * np.pi * hour / 24) + C_q * np.cos(4 * np.pi * hour / 24)\n","    return max(base_price * random.uniform(0.9, 1.1), 0)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1740749096876,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"},"user_tz":-120},"id":"Fzv96Gdu6wc_"},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","from gymnasium import spaces\n","import random\n","\n","class ElectricityMarketEnv(gym.Env):\n","    \"\"\"\n","    Custom Gym environment for an electricity market with battery storage.\n","    The agent decides when to charge/discharge to maximize profit.\n","    \"\"\"\n","    def __init__(self, max_timesteps=365, degradation_rate=0.99):\n","        super(ElectricityMarketEnv, self).__init__()\n","        self.timestep = 0\n","        self.max_timesteps = max_timesteps\n","        self.season = self.get_season_from_timestep(0)  # Set initial season based on real months\n","        self.episode_count = 0  # Track training episodes\n","        self.degradation_rate = degradation_rate  # Battery degradation factor\n","\n","        # Battery parameters\n","        self.initial_battery_capacity = 100  # Initial max storage capacity\n","        self.battery_capacity = self.initial_battery_capacity  # Maximum storage capacity\n","        self.battery_soc = 50  # Initial state of charge (SoC)\n","        self.efficiency = 0.95  # Charging/discharging efficiency\n","\n","        # Electricity market parameters\n","        self.base_min_price, self.base_max_price = self._calculate_price_bounds()\n","        self.base_min_demand, self.base_max_demand = self._calculate_demand_bounds()\n","\n","        self.min_price, self.max_price = self.base_min_price * 0.9, self.base_max_price * 1.1\n","        self.min_demand, self.max_demand = self.base_min_demand * 0.9, self.base_max_demand * 1.1\n","\n","\n","        # Action space: Charge (+) or discharge (-) within battery capacity\n","        self.action_space = spaces.Box(low=-self.battery_capacity, high=self.battery_capacity, shape=(1,), dtype=np.float32)\n","\n","        # Observation space: [Battery SoC, Battery Capacity, Electricity Price, Demand, Hour, Season]\n","        self.observation_space = spaces.Box(\n","            low=np.array([0, 50, self.min_price, self.min_demand, 0, 1], dtype=np.float32),\n","            high=np.array([self.initial_battery_capacity, self.initial_battery_capacity, self.max_price, self.max_demand, 23, 3], dtype=np.float32),\n","            dtype=np.float32\n","        )\n","\n","        self.reset()\n","\n","    def reset(self, seed=None, options=None):\n","        super().reset(seed=seed)\n","        self.battery_capacity = self.initial_battery_capacity  # Reset battery capacity\n","        self.timestep = 0\n","        self.battery_soc = 50  # Reset battery SoC\n","        self.season = self.get_season_from_timestep(self.timestep)  # Set season dynamically\n","        self.episode_count += 1  # Track episode count\n","        return self._get_state(), {}\n","\n","    def step(self, action):\n","        \"\"\"\n","        Executes one step in the environment.\n","        Action: Charge (>0) or discharge (<0) electricity.\n","        \"\"\"\n","        self.timestep += 1\n","        self.season = self.get_season_from_timestep(self.timestep)\n","        done = self.timestep >= self.max_timesteps or self.battery_soc <= 0\n","\n","        self.battery_capacity *= self.degradation_rate\n","        self.battery_capacity = max(self.battery_capacity, 50)  # Minimum capacity limit\n","\n","        # Get new price and demand based on season\n","        hour = self.timestep % 24\n","        demand = seasonal_duck_curve(hour, self.season)\n","        price = electricity_price_function(hour, self.season, demand)\n","\n","        # Clip action to valid range (-battery_capacity, +battery_capacity)\n","        action = np.clip(action[0], -self.battery_capacity, self.battery_capacity)\n","\n","        # Charge/discharge the battery\n","        if action > 0:  # Charging (cost money)\n","            charge_amount = min(action, self.battery_capacity - self.battery_soc)\n","            cost = charge_amount * price / self.efficiency\n","            self.battery_soc += charge_amount * self.efficiency\n","            reward = -cost  # Negative reward for spending money\n","            #print(f\"Step {self.timestep}, Season: {self.get_season_name(self.season)}: Charging {charge_amount:.2f} units at price {price:.2f}. Cost: {cost:.2f}. SoC: {self.battery_soc:.2f}.\")\n","        else:  # Discharging (sell to market)\n","            discharge_amount = min(-action, self.battery_soc, demand)\n","            revenue = discharge_amount * price * self.efficiency\n","            self.battery_soc -= discharge_amount / self.efficiency\n","            reward = revenue  # Positive reward for selling\n","            #print(f\"Step {self.timestep}, Season: {self.get_season_name(self.season)}: Discharging {discharge_amount:.2f} units at price {price:.2f}. Revenue: {revenue:.2f}. SoC: {self.battery_soc:.2f}.\")\n","\n","        next_state = np.array([self.battery_soc, self.battery_capacity, price, demand, hour, self.season], dtype=np.float32)\n","        return next_state, reward, done, False, {}\n","\n","    def _get_state(self):\n","        \"\"\" Returns the current state: [SoC, Price, Demand, Hour, Season] \"\"\"\n","        hour = self.timestep % 24\n","        demand = seasonal_duck_curve(hour, self.season)\n","        price = electricity_price_function(hour, self.season, demand)\n","        #print(f\"State - SoC: {self.battery_soc:.2f}, Price: {price:.2f}, Demand: {demand:.2f}, Hour: {hour}, Season: {self.season}.\")\n","        return np.array([self.battery_soc, self.battery_capacity, price, demand, hour, self.season], dtype=np.float32)\n","\n","\n","\n","    def get_season_from_timestep(self, timestep):\n","        \"\"\" Determines season based on day of the year \"\"\"\n","        month = (timestep // 30) % 12  # Approximate month from timestep\n","        if month in [11, 0, 1]:\n","            return 2  # Winter\n","        elif month in [5, 6, 7]:\n","            return 1  # Summer\n","        else:\n","            return 3  # Spring/Autumn\n","\n","    def get_season_name(self, season):\n","        return {1: \"Summer\", 2: \"Winter\", 3: \"Spring/Autumn\"}.get(season, \"Unknown\")\n","\n","\n","    def evaluate_agent(self, agent, num_episodes=3):\n","        \"\"\" Evaluates the agent over multiple episodes and prints performance metrics per season. \"\"\"\n","        total_rewards = []\n","        season_rewards = {1: [], 2: [], 3: []}\n","\n","        for ep in range(num_episodes):\n","            state, _ = self.reset()\n","            done = False\n","            episode_reward = 0\n","            seasonal_reward = {1: 0, 2: 0, 3: 0}\n","\n","            while not done:\n","                action, _ = agent.predict(state)\n","                state, reward, done, _, _ = self.step(action)\n","                episode_reward += reward\n","                seasonal_reward[self.season] += reward\n","\n","            total_rewards.append(episode_reward)\n","            for season in seasonal_reward:\n","                season_rewards[season].append(seasonal_reward[season])\n","\n","            print(f\"Episode {ep + 1}: Total Reward = {episode_reward:.2f}\")\n","\n","        avg_reward = np.mean(total_rewards)\n","        avg_seasonal_rewards = {season: np.mean(rewards) if rewards else 0 for season, rewards in season_rewards.items()}\n","\n","        print(f\"\\nEvaluation Results:\")\n","        print(f\"Average Reward over {num_episodes} episodes: {avg_reward:.2f}\")\n","        print(f\"Seasonal Performance:\")\n","        for season, avg in avg_seasonal_rewards.items():\n","            print(f\"  {self.get_season_name(season)}: {avg:.2f}\")\n","\n","        return avg_reward, avg_seasonal_rewards\n","\n","    def _calculate_price_bounds(self):\n","        \"\"\" Determine min/max price dynamically based on electricity_price_function \"\"\"\n","        min_price = float('inf')\n","        max_price = float('-inf')\n","        for season in [1, 2, 3]:\n","            for hour in range(24):\n","                demand = seasonal_duck_curve(hour, season)\n","                price = electricity_price_function(hour, season, demand)\n","                min_price = min(min_price, price)\n","                max_price = max(max_price, price)\n","        return min_price, max_price\n","\n","    def _calculate_demand_bounds(self):\n","        \"\"\" Determine min/max demand dynamically based on seasonal_duck_curve \"\"\"\n","        min_demand = float('inf')\n","        max_demand = float('-inf')\n","        for season in [1, 2, 3]:\n","            for hour in range(24):\n","                demand = seasonal_duck_curve(hour, season)\n","                min_demand = min(min_demand, demand)\n","                max_demand = max(max_demand, demand)\n","        return min_demand, max_demand\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1740749096878,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"},"user_tz":-120},"id":"1_WGxe9b65RP"},"outputs":[],"source":["env = ElectricityMarketEnv()\n","obs, _ = env.reset()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Fnk3VYfbSqdE"},"source":["#Evaluation Functions\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1740749096900,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"},"user_tz":-120},"id":"6TrhRFqqNHw_"},"outputs":[],"source":["from stable_baselines3.common.evaluation import evaluate_policy\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1890,"status":"ok","timestamp":1740749098799,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"},"user_tz":-120},"id":"HyQtsU2bP6Po","outputId":"2f711fcc-56f8-47a5-e463-5f001263c052"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.68.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (24.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (4.25.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (75.1.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"]}],"source":["%pip install tensorboard\n","%load_ext tensorboard\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1740749098864,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"},"user_tz":-120},"id":"6MO7nrFdGfkm"},"outputs":[],"source":["import time\n","import numpy as np\n","import tensorflow as tf\n","from stable_baselines3.common.callbacks import BaseCallback\n","\n","class RLComparisonCallback(BaseCallback):\n","    def __init__(self, verbose=0):\n","        super(RLComparisonCallback, self).__init__(verbose)\n","        self.start_time = time.time()\n","\n","    def _on_training_start(self) -> None:\n","        self.episode_rewards = []\n","        self.episode_lengths = []\n","        self.total_timesteps = 0\n","\n","    def _on_step(self) -> bool:\n","        if self.locals.get(\"dones\") is not None:\n","            for done, reward, info in zip(self.locals[\"dones\"], self.locals[\"rewards\"], self.locals[\"infos\"]):\n","                if done:\n","                    episode_reward = info.get(\"episode\", {}).get(\"r\", reward)\n","                    episode_length = info.get(\"episode\", {}).get(\"l\", 0)\n","\n","                    self.episode_rewards.append(episode_reward)\n","                    self.episode_lengths.append(episode_length)\n","                    self.total_timesteps += episode_length\n","\n","                    # Logging metrics after each episode using log scale\n","                    avg_reward = np.mean(self.episode_rewards[-100:])\n","                    cum_reward = np.sum(self.episode_rewards)\n","                    discounted_reward = np.sum([r * (0.99 ** i) for i, r in enumerate(self.episode_rewards)])\n","                    convergence_rate = np.std(self.episode_rewards[-100:])\n","                    sample_efficiency = cum_reward / max(1, self.total_timesteps)\n","                    stability = np.std(self.episode_rewards)\n","                    policy_entropy = info.get('entropy', 0)\n","                    time_complexity = time.time() - self.start_time\n","                    space_complexity = self.model.policy.parameters_to_vector().nbytes\n","\n","                    self.logger.record(\"custom/average_reward\", avg_reward)\n","                    self.logger.record(\"custom/cumulative_reward\", cum_reward)\n","                    self.logger.record(\"custom/discounted_reward\", discounted_reward)\n","                    self.logger.record(\"custom/convergence_rate\", convergence_rate)\n","                    self.logger.record(\"custom/sample_efficiency\", sample_efficiency)\n","                    self.logger.record(\"custom/stability\", stability)\n","                    self.logger.record(\"custom/policy_entropy\", policy_entropy)\n","                    self.logger.record(\"custom/time_complexity\", time_complexity)\n","                    self.logger.record(\"custom/space_complexity\", space_complexity)\n","\n","        return True\n"]},{"cell_type":"markdown","metadata":{"id":"GTxRkCSbIioU"},"source":["#Lets start with training"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1740749098887,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"},"user_tz":-120},"id":"K0hID_Uykm5J"},"outputs":[],"source":["SEEDS = [22, 68, 34, 90, 45]\n","steps = 1000000"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1740749098890,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"},"user_tz":-120},"id":"my5-4qwEIoSP"},"outputs":[],"source":["from stable_baselines3 import PPO, A2C, SAC, TD3, DDPG"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1740749098892,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"},"user_tz":-120},"id":"hZ3mFwHRm4eQ"},"outputs":[],"source":["from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList"]},{"cell_type":"markdown","metadata":{"id":"8zes4q1NaZMz"},"source":["#SPU"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":369,"status":"ok","timestamp":1740749099261,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"},"user_tz":-120},"id":"A9hzIYRBadLb"},"outputs":[],"source":["import time\n","from collections import deque\n","from itertools import count\n","\n","import numpy as np\n","import tensorflow as tf\n","from mpi4py import MPI\n","\n","from utils import logger\n","from utils import tf_util as U\n","from utils.Dataset import Dataset\n","from utils.mpi_adam import MpiAdam\n","from utils.utils import zipsame\n","\n","\n","def traj_segment_generator(pi, env, horizon, stochastic):\n","    t = 0\n","    ac = env.action_space.sample()  # not used, just so we have the datatype\n","    new = True  # marks if we're on first timestep of an episode\n","    ob = env.reset()\n","\n","    cur_ep_ret = 0  # return in current episode\n","    cur_ep_len = 0  # len of current episode\n","    ep_rets = []  # returns of completed episodes in this segment\n","    ep_lens = []  # lengths of ...\n","    unclipped_ep_rets = []\n","\n","    # Initialize history arrays\n","    obs = np.array([ob for _ in range(horizon)])\n","    rews = np.zeros(horizon, dtype='float32')\n","    vpreds = np.zeros(horizon, dtype='float32')\n","    news = np.zeros(horizon, dtype='int32')\n","    acs = np.array([ac for _ in range(horizon)])\n","    prevacs = acs.copy()\n","\n","    while True:\n","        prevac = ac\n","        ac, vpred = pi.act(stochastic, ob)\n","        if t > 0 and t % horizon == 0:\n","            yield {\"ob\": obs, \"rew\": rews, \"vpred\": vpreds, \"new\": news,\n","                   \"ac\": acs, \"prevac\": prevacs, \"nextvpred\": vpred * (1 - new),\n","                   \"ep_rets\": ep_rets, \"ep_lens\": ep_lens}\n","            ep_rets = []\n","            ep_lens = []\n","        i = t % horizon\n","        obs[i] = ob\n","        vpreds[i] = vpred\n","        news[i] = new\n","        acs[i] = ac\n","        prevacs[i] = prevac\n","\n","        ob, rew, new, info = env.step(ac)\n","\n","        rews[i] = rew\n","\n","        cur_ep_ret += rew\n","        cur_ep_len += 1\n","        if new:\n","            ep_rets.append(cur_ep_ret)\n","            ep_lens.append(cur_ep_len)\n","            cur_ep_ret = 0\n","            cur_ep_len = 0\n","            ob = env.reset()\n","        t += 1\n","\n","\n","def add_vtarg_and_adv(seg, gamma, lam):\n","    new = np.append(seg[\"new\"], 0)  # last element is only used for last vtarg, but we already zeroed it if last new = 1\n","    vpred = np.append(seg[\"vpred\"], seg[\"nextvpred\"])\n","    T = len(seg[\"rew\"])\n","    seg[\"adv\"] = gaelam = np.empty(T, dtype='float32')\n","    rew = seg[\"rew\"]\n","    lastgaelam = 0\n","    for t in reversed(range(T)):\n","        nonterminal = 1 - new[t + 1]\n","        delta = rew[t] + gamma * vpred[t + 1] * nonterminal - vpred[t]\n","        gaelam[t] = lastgaelam = delta + gamma * lam * nonterminal * lastgaelam\n","    seg[\"tdlamret\"] = seg[\"adv\"] + seg[\"vpred\"]\n","\n","\n","def learn(env, policy_fn, *,\n","          timesteps_per_actorbatch,  # timesteps per actor per update\n","          optim_stepsize, optim_batchsize,  # optimization hypers\n","          gamma, lam,  # advantage estimation\n","          entcoeff=0.0,\n","          max_episodes=0, max_iters=0, max_seconds=0,  # time constraint\n","          callback=None,  # you can do anything in the callback, since it takes locals(), globals()\n","          adam_epsilon=1e-5,\n","          schedule='constant',  # annealing for stepsize parameters (epsilon and adam)\n","          args\n","          ):\n","    # Setup losses and stuff\n","    ob_space = env.observation_space\n","    ac_space = env.action_space\n","    pi = policy_fn(\"pi\", ob_space, ac_space)  # Construct network for new policy\n","    oldpi = policy_fn(\"oldpi\", ob_space, ac_space)  # Network for old policy\n","\n","    # Ops to reassign params from new to old\n","    assign_old_eq_new = U.function([], [], updates=[tf.compat.v1.assign(oldv, newv)\n","                                                    for (oldv, newv) in\n","                                                    zipsame(oldpi.get_variables(), pi.get_variables())])\n","\n","    atarg = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])  # Target advantage function (if applicable)\n","    ret = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None])  # Empirical return\n","\n","    lrmult = tf.compat.v1.placeholder(name='lrmult', dtype=tf.float32,\n","                                      shape=[])  # learning rate multiplier, updated with schedule\n","\n","    ob = U.get_placeholder_cached(name=\"ob\")\n","    ac = pi.pdtype.sample_placeholder([None])\n","\n","    kloldnew = oldpi.pd.kl(pi.pd)\n","    ent = pi.pd.entropy()\n","    meankl = tf.reduce_mean(kloldnew)\n","    meanent = tf.reduce_mean(ent)\n","    pol_entpen = (-entcoeff) * meanent\n","\n","    newprob = tf.exp(pi.pd.logp(ac))\n","    oldprob = tf.exp(oldpi.pd.logp(ac))\n","\n","    ratio = newprob / oldprob\n","\n","    kl = pi.pd.kl(oldpi.pd)\n","    mean_kl = tf.reduce_mean(kl)\n","    get_kl = U.function([ob, ac], kl)\n","    get_mean_kl = U.function([ob, ac], mean_kl)\n","\n","    threshold = kl < args.kl_threshold\n","    threshold = tf.cast(threshold, tf.float32)\n","\n","    pol_surr = (kl - ratio * atarg / args.sepg_lam) * threshold\n","    pol_surr = tf.reduce_mean(pol_surr)\n","\n","    vf_loss = tf.reduce_mean(tf.square(pi.vpred - ret))\n","    total_loss = pol_surr + pol_entpen + vf_loss\n","    losses = [pol_surr, pol_entpen, vf_loss, meankl, meanent]\n","    loss_names = [\"pol_surr\", \"pol_entpen\", \"vf_loss\", \"kl\", \"ent\"]\n","\n","    var_list = pi.get_trainable_variables()\n","    lossandgrad = U.function([ob, ac, atarg, ret, lrmult],\n","                             losses + [U.flatgrad(total_loss, var_list)])\n","\n","    adam = MpiAdam(var_list, epsilon=adam_epsilon)\n","\n","    compute_losses = U.function([ob, ac, atarg, ret, lrmult], losses)\n","\n","    U.initialize()\n","    adam.sync()\n","\n","    # Prepare for rollouts\n","    seg_gen = traj_segment_generator(pi, env, timesteps_per_actorbatch, stochastic=True)\n","\n","    episodes_so_far = 0\n","    timesteps_so_far = 0\n","    iters_so_far = 0\n","    tstart = time.time()\n","    lenbuffer = deque(maxlen=100)  # rolling buffer for episode lengths\n","    rewbuffer = deque(maxlen=100)  # rolling buffer for episode rewards\n","\n","    running_scores = []\n","\n","    assert sum([max_iters > 0, args.num_timesteps > 0, max_episodes > 0,\n","                max_seconds > 0]) == 1, \"Only one time constraint permitted\"\n","\n","    while True:\n","        if callback:\n","            callback(locals(), globals())\n","        if args.num_timesteps and timesteps_so_far >= args.num_timesteps:\n","            break\n","        elif max_episodes and episodes_so_far >= max_episodes:\n","            break\n","        elif max_iters and iters_so_far >= max_iters:\n","            break\n","        elif max_seconds and time.time() - tstart >= max_seconds:\n","            break\n","\n","        if schedule == 'constant':\n","            cur_lrmult = 1.0\n","        elif schedule == 'linear':\n","            cur_lrmult = max(1.0 - float(timesteps_so_far) / args.num_timesteps, 0)\n","        else:\n","            raise NotImplementedError\n","\n","        if MPI.COMM_WORLD.Get_rank() == 0:\n","            logger.log(\"********** Iteration %i ************\" % iters_so_far)\n","\n","        seg = seg_gen.__next__()\n","        add_vtarg_and_adv(seg, gamma, lam)\n","\n","        ob, ac, atarg, tdlamret = seg[\"ob\"], seg[\"ac\"], seg[\"adv\"], seg[\"tdlamret\"]\n","        vpredbefore = seg[\"vpred\"]  # predicted value function before udpate\n","        atarg = (atarg - atarg.mean()) / (atarg.std() + 1e-8)  # standardized advantage function estimate\n","\n","        optim_batchsize = optim_batchsize or ob.shape[0]\n","\n","        if hasattr(pi, \"ob_rms\"): pi.ob_rms.update(ob)  # update running mean/std for policy\n","\n","        assign_old_eq_new()  # set old parameter values to new parameter values\n","\n","        d = Dataset(dict(ob=ob, ac=ac, atarg=atarg, vtarg=tdlamret), shuffle=not pi.recurrent)\n","\n","        # Optimization epochs over the data\n","        for num_epoch in count():\n","            losses = []  # list of tuples, each of which gives the loss for a minibatch\n","            for batch in d.iterate_once(optim_batchsize):\n","                *newlosses, g = lossandgrad(batch[\"ob\"], batch[\"ac\"],\n","                                            batch[\"atarg\"], batch[\"vtarg\"], cur_lrmult)\n","                g = np.nan_to_num(g)\n","                adam.update(g, optim_stepsize * cur_lrmult)\n","                losses.append(newlosses)\n","\n","            agg_mean_kl = get_mean_kl(ob, ac)\n","\n","            if agg_mean_kl > args.agg_kl_threshold or num_epoch == args.optim_epochs:\n","                break\n","\n","        lrlocal = (seg[\"ep_lens\"], seg[\"ep_rets\"])  # local values\n","        listoflrpairs = MPI.COMM_WORLD.allgather(lrlocal)  # list of tuples\n","        lens, rews = map(flatten_lists, zip(*listoflrpairs))\n","\n","        rewbuffer.extend(rews)\n","\n","        mean_score = None\n","\n","        if rewbuffer:\n","            mean_score = np.mean(rewbuffer)\n","            running_scores.append((timesteps_so_far, mean_score))\n","\n","        episodes_so_far += len(lens)\n","        timesteps_so_far += sum(lens)\n","        iters_so_far += 1\n","        if MPI.COMM_WORLD.Get_rank() == 0:\n","            logger.record_tabular(\"EpRewMean\", mean_score)\n","            logger.record_tabular(\"EpThisIter\", len(lens))\n","            logger.record_tabular(\"EpisodesSoFar\", episodes_so_far)\n","            logger.record_tabular(\"TimestepsSoFar\", timesteps_so_far)\n","            logger.record_tabular(\"TimeElapsed\", time.time() - tstart)\n","            logger.record_tabular(\"NumEpoch\", num_epoch)\n","\n","            logger.dump_tabular()\n","\n","    return pi\n","\n","\n","def flatten_lists(listoflists):\n","    return [el for list_ in listoflists for el in list_]\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"XA2cwv6Ja4Z1","executionInfo":{"status":"ok","timestamp":1740749099263,"user_tz":-120,"elapsed":1,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"}}},"outputs":[],"source":["import sys\n","import tensorflow as tf\n","from mpi4py import MPI\n","\n","from utils import logger\n","from utils import mlp_policy\n","from utils.utils import set_global_seeds, make_mujoco_env, mujoco_arg_parser, get_cpu_per_task, pkl_res\n","\n","def train(env, seed):\n","    env.reset(seed=seed)\n","    rank = MPI.COMM_WORLD.Get_rank()\n","\n","    ncpu = get_cpu_per_task()\n","    ncpu //= 8\n","\n","    sys.stdout.flush()\n","\n","    # Adjust TensorFlow session configuration for version 2.x\n","    # Using tf.compat.v1.Session() to ensure compatibility with older TensorFlow 1.x style code\n","    tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(\n","        allow_soft_placement=True,\n","        intra_op_parallelism_threads=ncpu,\n","        inter_op_parallelism_threads=ncpu\n","    )).as_default()\n","\n","    if rank == 0:\n","        logger.configure()\n","    else:\n","        logger.configure(format_strs=[])\n","\n","    workerseed = int(seed)\n","    #set_global_seeds(workerseed)\n","\n","    def policy_fn(name, ob_space, ac_space):\n","        return mlp_policy.MlpPolicy(name=name, ob_space=ob_space, ac_space=ac_space,\n","                                    hid_size=64, num_hid_layers=2)\n","\n","    # Assuming pposgd_simple.learn() returns the trained policy\n","    policy = learn(env, policy_fn,\n","                                         timesteps_per_actorbatch=2048,\n","                                         optim_stepsize=3e-4, optim_batchsize=64,\n","                                         gamma=0.99, lam=0.95, schedule='linear',\n","                                         args=[]\n","                                         )\n","\n","    return policy\n"]},{"cell_type":"code","source":["isinstance(env.observation_space, gym.spaces.Box)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bjWYuWf7r22D","executionInfo":{"status":"ok","timestamp":1740749099268,"user_tz":-120,"elapsed":4,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"}},"outputId":"d8eb476b-37c2-4256-e33f-fe3e00ff0dbf"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BmKKrsHXbLw_","executionInfo":{"status":"error","timestamp":1740749099518,"user_tz":-120,"elapsed":249,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"}},"outputId":"14a31d21-8615-4f1f-9e8e-ff66311ccbed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to /tmp/openai-2025-02-28-13-24-59-219990\n"]},{"output_type":"error","ename":"ValueError","evalue":"A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-9209cddd0289>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-73c474db6fde>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Assuming pposgd_simple.learn() returns the trained policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     policy = learn(env, policy_fn,\n\u001b[0m\u001b[1;32m     40\u001b[0m                                          \u001b[0mtimesteps_per_actorbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                          \u001b[0moptim_stepsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_batchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-d622ab3ad0b1>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(env, policy_fn, timesteps_per_actorbatch, optim_stepsize, optim_batchsize, gamma, lam, entcoeff, max_episodes, max_iters, max_seconds, callback, adam_epsilon, schedule, args)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mob_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mac_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac_space\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Construct network for new policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0moldpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"oldpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac_space\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Network for old policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-73c474db6fde>\u001b[0m in \u001b[0;36mpolicy_fn\u001b[0;34m(name, ob_space, ac_space)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpolicy_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         return mlp_policy.MlpPolicy(name=name, ob_space=ob_space, ac_space=ac_space,\n\u001b[0m\u001b[1;32m     36\u001b[0m                                     hid_size=64, num_hid_layers=2)\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils/mlp_policy.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMlpPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initialize with the module name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hid_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaussian_fixed_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/utils/mlp_policy.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, ob_space, ac_space, hid_size, num_hid_layers, gaussian_fixed_var)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Value function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mobz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mob_rms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mob_rms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mlast_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hid_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/common/keras_tensor.py\u001b[0m in \u001b[0;36m__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__tf_tensor__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;34m\"A KerasTensor cannot be used as input to a TensorFlow function. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;34m\"A KerasTensor is a symbolic placeholder for a shape and dtype, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"]}],"source":["pi = train(env, 22)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BckKeL8ucEu_"},"outputs":[],"source":["env.evaluate_agent(pi, num_episodes=365)"]},{"cell_type":"markdown","metadata":{"id":"DePjCYobm4Q6"},"source":["#PPO"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WMq3x0ujIc1E"},"outputs":[],"source":["\n","model_type = \"PPO\"\n","for seed in SEEDS:\n","  env.reset(seed=seed)\n","\n","  checkpoint_callback = CheckpointCallback(\n","      save_freq=200000,  # Save every 100,000 steps\n","      save_path='./models/',  # Directory to save the model\n","      name_prefix=f\"{model_type}_{seed}_{steps}\"  # Prefix for the saved model files\n","  )\n","  ppo_callback = CallbackList([RLComparisonCallback(), checkpoint_callback])\n","\n","  ppo_model = PPO(\"MlpPolicy\", env, verbose=0, tensorboard_log=\"./tensorboard/\",  seed=seed).learn(steps, callback=ppo_callback, tb_log_name=f\"{model_type}_{seed}_{steps}\")\n","  ppo_model.save(f\"./models/{model_type}_{seed}_{steps}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bDPiQjtqaw3_"},"outputs":[],"source":["env.evaluate_agent(ppo_model, num_episodes=365)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyON2Qd8P36BD3Vd0ThGAYNu"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}