{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPLNF2KsbjXE1ulgJXyIFjg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wo7FwA5zLw2F"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["#Setup"],"metadata":{"id":"kChfkhkDWFvs"}},{"cell_type":"code","source":["!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n","!pip install \"stable_baselines3==2.5.0\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LcR0xq9ZWI9g","executionInfo":{"status":"ok","timestamp":1740694225663,"user_tz":-120,"elapsed":11061,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"}},"outputId":"d376d1ff-9cbe-4edb-b12a-b17991b186cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","freeglut3-dev is already the newest version (2.8.1-6).\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.13).\n","0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n","Requirement already satisfied: stable-baselines3>=2.0.0a4 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.6.0a1)\n","Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.0.0)\n","Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.2.3)\n","Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.6.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.1.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.10.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (4.11.0.86)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.6.1)\n","Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (2.18.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (5.9.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (4.67.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (13.9.4)\n","Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (0.10.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a4) (11.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.0.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.70.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (24.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (4.25.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (75.1.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.1.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.17.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3>=2.0.0a4->stable-baselines3[extra]>=2.0.0a4) (2025.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a4) (0.1.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.0.2)\n"]}]},{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","from gymnasium import spaces\n","from stable_baselines3.common.env_checker import check_env\n"],"metadata":{"id":"gVd8rL1W6CJf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Environment"],"metadata":{"id":"XNRk05DSVz4H"}},{"cell_type":"code","source":["def seasonal_duck_curve(hour, season, noise_scale=0.03):\n","    \"\"\" Computes electricity demand based on seasonal duck curve \"\"\"\n","    A = 400\n","    seasonal_params = {\n","        1: (100, 7, 80, 12, 120, 14, 250, 18, 30, 4),  # Summer\n","        2: (180, 6, 40, 12, 80, 14, 220, 17, 60, 4),  # Winter\n","        3: (130, 7, 90, 12, 100, 14, 180, 18, 40, 4)  # Spring/Autumn\n","    }\n","    B, t_morning, C, t_dip, D, mu_dip, E, t_evening, F, t_early = seasonal_params[season]\n","    demand = A + B / (1 + np.exp(-1 * (hour - t_morning))) - C / (1 + np.exp(-1 * (hour - t_dip))) - D * np.exp(-((hour - mu_dip)**2) / 4) + E / (1 + np.exp(-1 * (hour - t_evening)))\n","    return demand * random.uniform(0.9, 1.1)\n","\n","def electricity_price_function(hour, season, demand, noise_scale=0.03):\n","    \"\"\" Computes electricity price based on seasonal demand \"\"\"\n","    season_params = {\n","        1: (30, 15, 10),  # Summer\n","        2: (28, 14, 9),   # Winter\n","        3: (25, 12, 8)    # Spring/Autumn\n","    }\n","    A_q, B_q, C_q = season_params[season]\n","    base_price = A_q + B_q * np.cos(2 * np.pi * hour / 24) + C_q * np.cos(4 * np.pi * hour / 24)\n","    return max(base_price * random.uniform(0.9, 1.1), 0)\n"],"metadata":{"id":"TFNOOYSFgg3X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","from gymnasium import spaces\n","import random\n","\n","class ElectricityMarketEnv(gym.Env):\n","    \"\"\"\n","    Custom Gym environment for an electricity market with battery storage.\n","    The agent decides when to charge/discharge to maximize profit.\n","    \"\"\"\n","    def __init__(self, max_timesteps=365, degradation_rate=0.99):\n","        super(ElectricityMarketEnv, self).__init__()\n","        self.timestep = 0\n","        self.max_timesteps = max_timesteps\n","        self.season = self.get_season_from_timestep(0)  # Set initial season based on real months\n","        self.episode_count = 0  # Track training episodes\n","        self.degradation_rate = degradation_rate  # Battery degradation factor\n","\n","        # Battery parameters\n","        self.initial_battery_capacity = 100  # Initial max storage capacity\n","        self.battery_capacity = self.initial_battery_capacity  # Maximum storage capacity\n","        self.battery_soc = 50  # Initial state of charge (SoC)\n","        self.efficiency = 0.95  # Charging/discharging efficiency\n","\n","        # Electricity market parameters\n","        self.base_min_price, self.base_max_price = self._calculate_price_bounds()\n","        self.base_min_demand, self.base_max_demand = self._calculate_demand_bounds()\n","\n","        self.min_price, self.max_price = self.base_min_price * 0.9, self.base_max_price * 1.1\n","        self.min_demand, self.max_demand = self.base_min_demand * 0.9, self.base_max_demand * 1.1\n","\n","\n","        # Action space: Charge (+) or discharge (-) within battery capacity\n","        self.action_space = spaces.Box(low=-self.battery_capacity, high=self.battery_capacity, shape=(1,), dtype=np.float32)\n","\n","        # Observation space: [Battery SoC, Battery Capacity, Electricity Price, Demand, Hour, Season]\n","        self.observation_space = spaces.Box(\n","            low=np.array([0, 50, self.min_price, self.min_demand, 0, 1], dtype=np.float32),\n","            high=np.array([self.initial_battery_capacity, self.initial_battery_capacity, self.max_price, self.max_demand, 23, 3], dtype=np.float32),\n","            dtype=np.float32\n","        )\n","\n","        self.reset()\n","\n","    def reset(self, seed=None, options=None):\n","        super().reset(seed=seed)\n","        self.battery_capacity = self.initial_battery_capacity  # Reset battery capacity\n","        self.timestep = 0\n","        self.battery_soc = 50  # Reset battery SoC\n","        self.season = self.get_season_from_timestep(self.timestep)  # Set season dynamically\n","        self.episode_count += 1  # Track episode count\n","        return self._get_state(), {}\n","\n","    def step(self, action):\n","        \"\"\"\n","        Executes one step in the environment.\n","        Action: Charge (>0) or discharge (<0) electricity.\n","        \"\"\"\n","        self.timestep += 1\n","        self.season = self.get_season_from_timestep(self.timestep)\n","        done = self.timestep >= self.max_timesteps or self.battery_soc <= 0\n","\n","        self.battery_capacity *= self.degradation_rate\n","        self.battery_capacity = max(self.battery_capacity, 50)  # Minimum capacity limit\n","\n","        # Get new price and demand based on season\n","        hour = self.timestep % 24\n","        demand = seasonal_duck_curve(hour, self.season)\n","        price = electricity_price_function(hour, self.season, demand)\n","\n","        # Clip action to valid range (-battery_capacity, +battery_capacity)\n","        action = np.clip(action[0], -self.battery_capacity, self.battery_capacity)\n","\n","        # Charge/discharge the battery\n","        if action > 0:  # Charging (cost money)\n","            charge_amount = min(action, self.battery_capacity - self.battery_soc)\n","            cost = charge_amount * price / self.efficiency\n","            self.battery_soc += charge_amount * self.efficiency\n","            reward = -cost  # Negative reward for spending money\n","            #print(f\"Step {self.timestep}, Season: {self.get_season_name(self.season)}: Charging {charge_amount:.2f} units at price {price:.2f}. Cost: {cost:.2f}. SoC: {self.battery_soc:.2f}.\")\n","        else:  # Discharging (sell to market)\n","            discharge_amount = min(-action, self.battery_soc, demand)\n","            revenue = discharge_amount * price * self.efficiency\n","            self.battery_soc -= discharge_amount / self.efficiency\n","            reward = revenue  # Positive reward for selling\n","            #print(f\"Step {self.timestep}, Season: {self.get_season_name(self.season)}: Discharging {discharge_amount:.2f} units at price {price:.2f}. Revenue: {revenue:.2f}. SoC: {self.battery_soc:.2f}.\")\n","\n","        next_state = np.array([self.battery_soc, self.battery_capacity, price, demand, hour, self.season], dtype=np.float32)\n","        return next_state, reward, done, False, {}\n","\n","    def _get_state(self):\n","        \"\"\" Returns the current state: [SoC, Price, Demand, Hour, Season] \"\"\"\n","        hour = self.timestep % 24\n","        demand = seasonal_duck_curve(hour, self.season)\n","        price = electricity_price_function(hour, self.season, demand)\n","        #print(f\"State - SoC: {self.battery_soc:.2f}, Price: {price:.2f}, Demand: {demand:.2f}, Hour: {hour}, Season: {self.season}.\")\n","        return np.array([self.battery_soc, self.battery_capacity, price, demand, hour, self.season], dtype=np.float32)\n","\n","\n","\n","    def get_season_from_timestep(self, timestep):\n","        \"\"\" Determines season based on day of the year \"\"\"\n","        month = (timestep // 30) % 12  # Approximate month from timestep\n","        if month in [11, 0, 1]:\n","            return 2  # Winter\n","        elif month in [5, 6, 7]:\n","            return 1  # Summer\n","        else:\n","            return 3  # Spring/Autumn\n","\n","    def get_season_name(self, season):\n","        return {1: \"Summer\", 2: \"Winter\", 3: \"Spring/Autumn\"}.get(season, \"Unknown\")\n","\n","\n","    def evaluate_agent(self, agent, num_episodes=3):\n","        \"\"\" Evaluates the agent over multiple episodes and prints performance metrics per season. \"\"\"\n","        total_rewards = []\n","        season_rewards = {1: [], 2: [], 3: []}\n","\n","        for ep in range(num_episodes):\n","            state, _ = self.reset()\n","            done = False\n","            episode_reward = 0\n","            seasonal_reward = {1: 0, 2: 0, 3: 0}\n","\n","            while not done:\n","                action, _ = agent.predict(state)\n","                state, reward, done, _, _ = self.step(action)\n","                episode_reward += reward\n","                seasonal_reward[self.season] += reward\n","\n","            total_rewards.append(episode_reward)\n","            for season in seasonal_reward:\n","                season_rewards[season].append(seasonal_reward[season])\n","\n","            print(f\"Episode {ep + 1}: Total Reward = {episode_reward:.2f}\")\n","\n","        avg_reward = np.mean(total_rewards)\n","        avg_seasonal_rewards = {season: np.mean(rewards) if rewards else 0 for season, rewards in season_rewards.items()}\n","\n","        print(f\"\\nEvaluation Results:\")\n","        print(f\"Average Reward over {num_episodes} episodes: {avg_reward:.2f}\")\n","        print(f\"Seasonal Performance:\")\n","        for season, avg in avg_seasonal_rewards.items():\n","            print(f\"  {self.get_season_name(season)}: {avg:.2f}\")\n","\n","        return avg_reward, avg_seasonal_rewards\n","\n","    def _calculate_price_bounds(self):\n","        \"\"\" Determine min/max price dynamically based on electricity_price_function \"\"\"\n","        min_price = float('inf')\n","        max_price = float('-inf')\n","        for season in [1, 2, 3]:\n","            for hour in range(24):\n","                demand = seasonal_duck_curve(hour, season)\n","                price = electricity_price_function(hour, season, demand)\n","                min_price = min(min_price, price)\n","                max_price = max(max_price, price)\n","        return min_price, max_price\n","\n","    def _calculate_demand_bounds(self):\n","        \"\"\" Determine min/max demand dynamically based on seasonal_duck_curve \"\"\"\n","        min_demand = float('inf')\n","        max_demand = float('-inf')\n","        for season in [1, 2, 3]:\n","            for hour in range(24):\n","                demand = seasonal_duck_curve(hour, season)\n","                min_demand = min(min_demand, demand)\n","                max_demand = max(max_demand, demand)\n","        return min_demand, max_demand\n","\n"],"metadata":{"id":"Fzv96Gdu6wc_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = ElectricityMarketEnv()\n","obs, _ = env.reset()\n","\n"],"metadata":{"id":"1_WGxe9b65RP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fnk3VYfbSqdE"},"source":["#Evaluation Functions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6TrhRFqqNHw_"},"outputs":[],"source":["from stable_baselines3.common.evaluation import evaluate_policy\n"]},{"cell_type":"markdown","metadata":{"id":"GTxRkCSbIioU"},"source":["#Lets start with training"]},{"cell_type":"code","source":["SEEDS = [22, 68, 34, 90, 45]\n","steps = 1000000"],"metadata":{"id":"K0hID_Uykm5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"my5-4qwEIoSP"},"outputs":[],"source":["from stable_baselines3 import PPO, A2C, SAC, TD3, DDPG"]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList"],"metadata":{"id":"hZ3mFwHRm4eQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DDPG"],"metadata":{"id":"jAFryZdAn_uq"}},{"cell_type":"code","source":["!pip install --upgrade numpy cloudpickle\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8RS44qF6aSFT","executionInfo":{"status":"ok","timestamp":1740694253694,"user_tz":-120,"elapsed":9023,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"}},"outputId":"150558e9-f73c-4edb-fcd5-e70461a8b67c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.3)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (3.1.1)\n"]}]},{"cell_type":"code","source":["!pip install torch==2.6.0\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RHhwunWqcQs3","executionInfo":{"status":"ok","timestamp":1740694259609,"user_tz":-120,"elapsed":5913,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"}},"outputId":"456b2ef6-0aad-4c99-e04c-2b61c6463eee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (2.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0) (3.0.2)\n"]}]},{"cell_type":"code","source":["\n","model_type = \"DDPG\"\n","ddpg = DDPG(\"MlpPolicy\", env, verbose=0)\n","ppo_model = DDPG.load(\"./DDPG_22_1000000_800000_steps\", env=env, custom_objects={'observation_space': ddpg.observation_space, 'action_space': ddpg.action_space})\n","env.evaluate_agent(ppo_model, num_episodes=365)"],"metadata":{"id":"CxcdfDqIn_uq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740694422961,"user_tz":-120,"elapsed":424,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"}},"outputId":"b52abb3e-bcdc-4f8e-829a-754b7c33e135"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1: Total Reward = 2473.09\n","Episode 2: Total Reward = 2109.35\n","Episode 3: Total Reward = 2439.66\n","Episode 4: Total Reward = 2083.98\n","Episode 5: Total Reward = 2259.57\n","Episode 6: Total Reward = 2359.34\n","Episode 7: Total Reward = 2082.73\n","Episode 8: Total Reward = 2235.52\n","Episode 9: Total Reward = 2267.57\n","Episode 10: Total Reward = 2233.16\n","Episode 11: Total Reward = 2431.97\n","Episode 12: Total Reward = 2080.42\n","Episode 13: Total Reward = 2007.47\n","Episode 14: Total Reward = 2013.83\n","Episode 15: Total Reward = 2228.43\n","Episode 16: Total Reward = 2340.74\n","Episode 17: Total Reward = 2438.43\n","Episode 18: Total Reward = 2326.15\n","Episode 19: Total Reward = 2057.54\n","Episode 20: Total Reward = 2048.88\n","Episode 21: Total Reward = 2390.29\n","Episode 22: Total Reward = 2220.73\n","Episode 23: Total Reward = 2114.55\n","Episode 24: Total Reward = 2299.41\n","Episode 25: Total Reward = 2021.80\n","Episode 26: Total Reward = 2114.10\n","Episode 27: Total Reward = 1997.88\n","Episode 28: Total Reward = 2247.31\n","Episode 29: Total Reward = 2071.08\n","Episode 30: Total Reward = 2077.65\n","Episode 31: Total Reward = 2098.13\n","Episode 32: Total Reward = 2449.69\n","Episode 33: Total Reward = 2346.46\n","Episode 34: Total Reward = 2050.11\n","Episode 35: Total Reward = 2184.41\n","Episode 36: Total Reward = 2330.63\n","Episode 37: Total Reward = 2098.98\n","Episode 38: Total Reward = 2048.83\n","Episode 39: Total Reward = 2166.94\n","Episode 40: Total Reward = 2162.27\n","Episode 41: Total Reward = 2116.79\n","Episode 42: Total Reward = 2043.12\n","Episode 43: Total Reward = 2173.25\n","Episode 44: Total Reward = 2314.43\n","Episode 45: Total Reward = 2036.22\n","Episode 46: Total Reward = 2305.37\n","Episode 47: Total Reward = 2099.22\n","Episode 48: Total Reward = 2069.40\n","Episode 49: Total Reward = 2409.87\n","Episode 50: Total Reward = 2277.56\n","Episode 51: Total Reward = 2461.72\n","Episode 52: Total Reward = 2022.55\n","Episode 53: Total Reward = 2339.51\n","Episode 54: Total Reward = 1993.98\n","Episode 55: Total Reward = 2074.85\n","Episode 56: Total Reward = 2232.84\n","Episode 57: Total Reward = 2095.71\n","Episode 58: Total Reward = 2374.42\n","Episode 59: Total Reward = 2333.11\n","Episode 60: Total Reward = 2250.21\n","Episode 61: Total Reward = 2327.63\n","Episode 62: Total Reward = 2174.40\n","Episode 63: Total Reward = 2291.28\n","Episode 64: Total Reward = 2372.02\n","Episode 65: Total Reward = 2044.08\n","Episode 66: Total Reward = 2310.81\n","Episode 67: Total Reward = 2094.62\n","Episode 68: Total Reward = 2333.31\n","Episode 69: Total Reward = 2228.68\n","Episode 70: Total Reward = 2060.68\n","Episode 71: Total Reward = 2072.73\n","Episode 72: Total Reward = 2360.96\n","Episode 73: Total Reward = 2336.59\n","Episode 74: Total Reward = 2187.42\n","Episode 75: Total Reward = 2452.76\n","Episode 76: Total Reward = 2043.06\n","Episode 77: Total Reward = 2366.01\n","Episode 78: Total Reward = 2119.10\n","Episode 79: Total Reward = 2107.38\n","Episode 80: Total Reward = 2313.57\n","Episode 81: Total Reward = 2380.68\n","Episode 82: Total Reward = 2040.83\n","Episode 83: Total Reward = 2038.74\n","Episode 84: Total Reward = 2129.94\n","Episode 85: Total Reward = 2118.00\n","Episode 86: Total Reward = 2339.90\n","Episode 87: Total Reward = 2116.05\n","Episode 88: Total Reward = 2251.57\n","Episode 89: Total Reward = 2310.84\n","Episode 90: Total Reward = 2456.72\n","Episode 91: Total Reward = 2166.38\n","Episode 92: Total Reward = 2165.01\n","Episode 93: Total Reward = 2018.43\n","Episode 94: Total Reward = 2222.61\n","Episode 95: Total Reward = 2028.26\n","Episode 96: Total Reward = 2231.65\n","Episode 97: Total Reward = 2392.68\n","Episode 98: Total Reward = 2394.90\n","Episode 99: Total Reward = 2333.47\n","Episode 100: Total Reward = 2376.46\n","Episode 101: Total Reward = 2130.13\n","Episode 102: Total Reward = 2286.61\n","Episode 103: Total Reward = 2355.51\n","Episode 104: Total Reward = 2216.08\n","Episode 105: Total Reward = 2240.43\n","Episode 106: Total Reward = 2141.57\n","Episode 107: Total Reward = 2195.26\n","Episode 108: Total Reward = 2294.61\n","Episode 109: Total Reward = 2225.04\n","Episode 110: Total Reward = 2328.35\n","Episode 111: Total Reward = 2430.76\n","Episode 112: Total Reward = 2044.09\n","Episode 113: Total Reward = 2096.59\n","Episode 114: Total Reward = 2237.20\n","Episode 115: Total Reward = 2054.40\n","Episode 116: Total Reward = 2013.11\n","Episode 117: Total Reward = 2320.38\n","Episode 118: Total Reward = 2378.87\n","Episode 119: Total Reward = 2416.26\n","Episode 120: Total Reward = 2303.79\n","Episode 121: Total Reward = 2250.38\n","Episode 122: Total Reward = 2275.26\n","Episode 123: Total Reward = 2074.01\n","Episode 124: Total Reward = 2290.44\n","Episode 125: Total Reward = 2337.44\n","Episode 126: Total Reward = 2251.02\n","Episode 127: Total Reward = 2270.94\n","Episode 128: Total Reward = 2057.49\n","Episode 129: Total Reward = 2354.25\n","Episode 130: Total Reward = 1991.10\n","Episode 131: Total Reward = 2182.03\n","Episode 132: Total Reward = 2430.99\n","Episode 133: Total Reward = 2347.88\n","Episode 134: Total Reward = 2057.65\n","Episode 135: Total Reward = 2315.41\n","Episode 136: Total Reward = 2362.14\n","Episode 137: Total Reward = 2214.14\n","Episode 138: Total Reward = 2465.19\n","Episode 139: Total Reward = 2223.33\n","Episode 140: Total Reward = 2180.74\n","Episode 141: Total Reward = 2193.02\n","Episode 142: Total Reward = 2356.15\n","Episode 143: Total Reward = 2208.61\n","Episode 144: Total Reward = 2351.80\n","Episode 145: Total Reward = 2346.53\n","Episode 146: Total Reward = 2158.57\n","Episode 147: Total Reward = 2332.92\n","Episode 148: Total Reward = 2325.95\n","Episode 149: Total Reward = 2230.01\n","Episode 150: Total Reward = 2286.10\n","Episode 151: Total Reward = 2003.21\n","Episode 152: Total Reward = 2446.69\n","Episode 153: Total Reward = 2167.83\n","Episode 154: Total Reward = 2178.80\n","Episode 155: Total Reward = 2099.69\n","Episode 156: Total Reward = 2059.93\n","Episode 157: Total Reward = 2207.12\n","Episode 158: Total Reward = 2024.62\n","Episode 159: Total Reward = 2209.19\n","Episode 160: Total Reward = 2291.04\n","Episode 161: Total Reward = 2104.62\n","Episode 162: Total Reward = 2299.51\n","Episode 163: Total Reward = 2208.68\n","Episode 164: Total Reward = 2093.20\n","Episode 165: Total Reward = 2416.34\n","Episode 166: Total Reward = 2423.95\n","Episode 167: Total Reward = 2375.07\n","Episode 168: Total Reward = 2372.48\n","Episode 169: Total Reward = 2136.62\n","Episode 170: Total Reward = 1996.78\n","Episode 171: Total Reward = 2119.20\n","Episode 172: Total Reward = 2422.81\n","Episode 173: Total Reward = 2050.38\n","Episode 174: Total Reward = 2062.06\n","Episode 175: Total Reward = 2434.10\n","Episode 176: Total Reward = 2078.57\n","Episode 177: Total Reward = 2297.31\n","Episode 178: Total Reward = 2139.00\n","Episode 179: Total Reward = 2454.11\n","Episode 180: Total Reward = 2335.26\n","Episode 181: Total Reward = 2032.58\n","Episode 182: Total Reward = 2299.69\n","Episode 183: Total Reward = 2406.92\n","Episode 184: Total Reward = 2401.82\n","Episode 185: Total Reward = 2011.41\n","Episode 186: Total Reward = 2311.04\n","Episode 187: Total Reward = 2336.91\n","Episode 188: Total Reward = 2015.24\n","Episode 189: Total Reward = 2126.72\n","Episode 190: Total Reward = 2421.78\n","Episode 191: Total Reward = 2169.11\n","Episode 192: Total Reward = 2329.81\n","Episode 193: Total Reward = 2277.44\n","Episode 194: Total Reward = 1997.54\n","Episode 195: Total Reward = 2011.88\n","Episode 196: Total Reward = 2290.86\n","Episode 197: Total Reward = 2269.71\n","Episode 198: Total Reward = 2383.02\n","Episode 199: Total Reward = 2015.02\n","Episode 200: Total Reward = 2360.17\n","Episode 201: Total Reward = 2384.32\n","Episode 202: Total Reward = 2379.10\n","Episode 203: Total Reward = 2432.46\n","Episode 204: Total Reward = 2264.71\n","Episode 205: Total Reward = 2113.51\n","Episode 206: Total Reward = 2162.89\n","Episode 207: Total Reward = 2371.14\n","Episode 208: Total Reward = 2048.68\n","Episode 209: Total Reward = 2200.69\n","Episode 210: Total Reward = 2271.96\n","Episode 211: Total Reward = 2297.89\n","Episode 212: Total Reward = 2335.56\n","Episode 213: Total Reward = 2093.34\n","Episode 214: Total Reward = 2297.90\n","Episode 215: Total Reward = 2332.78\n","Episode 216: Total Reward = 2085.10\n","Episode 217: Total Reward = 2275.78\n","Episode 218: Total Reward = 1991.25\n","Episode 219: Total Reward = 2365.85\n","Episode 220: Total Reward = 2023.50\n","Episode 221: Total Reward = 2298.52\n","Episode 222: Total Reward = 2383.66\n","Episode 223: Total Reward = 2262.15\n","Episode 224: Total Reward = 2074.83\n","Episode 225: Total Reward = 2309.84\n","Episode 226: Total Reward = 2288.32\n","Episode 227: Total Reward = 2181.65\n","Episode 228: Total Reward = 2053.82\n","Episode 229: Total Reward = 2237.00\n","Episode 230: Total Reward = 2422.24\n","Episode 231: Total Reward = 2388.15\n","Episode 232: Total Reward = 2004.83\n","Episode 233: Total Reward = 2410.39\n","Episode 234: Total Reward = 2032.97\n","Episode 235: Total Reward = 2020.03\n","Episode 236: Total Reward = 2128.77\n","Episode 237: Total Reward = 2221.24\n","Episode 238: Total Reward = 2006.59\n","Episode 239: Total Reward = 2152.93\n","Episode 240: Total Reward = 2401.48\n","Episode 241: Total Reward = 2407.05\n","Episode 242: Total Reward = 2294.56\n","Episode 243: Total Reward = 2332.73\n","Episode 244: Total Reward = 2294.00\n","Episode 245: Total Reward = 2283.86\n","Episode 246: Total Reward = 2022.19\n","Episode 247: Total Reward = 2372.86\n","Episode 248: Total Reward = 2064.55\n","Episode 249: Total Reward = 2046.74\n","Episode 250: Total Reward = 2287.75\n","Episode 251: Total Reward = 2208.28\n","Episode 252: Total Reward = 2230.92\n","Episode 253: Total Reward = 2416.47\n","Episode 254: Total Reward = 2362.29\n","Episode 255: Total Reward = 2018.91\n","Episode 256: Total Reward = 2289.06\n","Episode 257: Total Reward = 2053.53\n","Episode 258: Total Reward = 2233.66\n","Episode 259: Total Reward = 2185.23\n","Episode 260: Total Reward = 2021.60\n","Episode 261: Total Reward = 2012.34\n","Episode 262: Total Reward = 2388.42\n","Episode 263: Total Reward = 2418.21\n","Episode 264: Total Reward = 2015.92\n","Episode 265: Total Reward = 2317.53\n","Episode 266: Total Reward = 2264.69\n","Episode 267: Total Reward = 2037.41\n","Episode 268: Total Reward = 2374.99\n","Episode 269: Total Reward = 2308.88\n","Episode 270: Total Reward = 2193.02\n","Episode 271: Total Reward = 2363.79\n","Episode 272: Total Reward = 2084.73\n","Episode 273: Total Reward = 2040.93\n","Episode 274: Total Reward = 2396.10\n","Episode 275: Total Reward = 2054.70\n","Episode 276: Total Reward = 2427.47\n","Episode 277: Total Reward = 2301.29\n","Episode 278: Total Reward = 2105.10\n","Episode 279: Total Reward = 2323.88\n","Episode 280: Total Reward = 2099.18\n","Episode 281: Total Reward = 2412.97\n","Episode 282: Total Reward = 2148.85\n","Episode 283: Total Reward = 2425.87\n","Episode 284: Total Reward = 2328.27\n","Episode 285: Total Reward = 2344.55\n","Episode 286: Total Reward = 2081.46\n","Episode 287: Total Reward = 2096.86\n","Episode 288: Total Reward = 2153.02\n","Episode 289: Total Reward = 2245.44\n","Episode 290: Total Reward = 2280.10\n","Episode 291: Total Reward = 2205.87\n","Episode 292: Total Reward = 2103.72\n","Episode 293: Total Reward = 2220.59\n","Episode 294: Total Reward = 2239.58\n","Episode 295: Total Reward = 2249.64\n","Episode 296: Total Reward = 2415.57\n","Episode 297: Total Reward = 2060.74\n","Episode 298: Total Reward = 2150.99\n","Episode 299: Total Reward = 2269.09\n","Episode 300: Total Reward = 2078.16\n","Episode 301: Total Reward = 2440.50\n","Episode 302: Total Reward = 2186.30\n","Episode 303: Total Reward = 2383.67\n","Episode 304: Total Reward = 2009.32\n","Episode 305: Total Reward = 2117.76\n","Episode 306: Total Reward = 2076.45\n","Episode 307: Total Reward = 2196.68\n","Episode 308: Total Reward = 2321.17\n","Episode 309: Total Reward = 2239.04\n","Episode 310: Total Reward = 2067.97\n","Episode 311: Total Reward = 2137.10\n","Episode 312: Total Reward = 2165.00\n","Episode 313: Total Reward = 2014.17\n","Episode 314: Total Reward = 2370.25\n","Episode 315: Total Reward = 2227.81\n","Episode 316: Total Reward = 2368.25\n","Episode 317: Total Reward = 2262.40\n","Episode 318: Total Reward = 2357.98\n","Episode 319: Total Reward = 2337.38\n","Episode 320: Total Reward = 2222.10\n","Episode 321: Total Reward = 2460.20\n","Episode 322: Total Reward = 2261.86\n","Episode 323: Total Reward = 2022.13\n","Episode 324: Total Reward = 2099.98\n","Episode 325: Total Reward = 2390.78\n","Episode 326: Total Reward = 2033.54\n","Episode 327: Total Reward = 2401.90\n","Episode 328: Total Reward = 2103.06\n","Episode 329: Total Reward = 2028.21\n","Episode 330: Total Reward = 2370.74\n","Episode 331: Total Reward = 2157.95\n","Episode 332: Total Reward = 2322.46\n","Episode 333: Total Reward = 2214.03\n","Episode 334: Total Reward = 2380.01\n","Episode 335: Total Reward = 2271.63\n","Episode 336: Total Reward = 2402.54\n","Episode 337: Total Reward = 2004.60\n","Episode 338: Total Reward = 2419.60\n","Episode 339: Total Reward = 2214.36\n","Episode 340: Total Reward = 2091.17\n","Episode 341: Total Reward = 2092.36\n","Episode 342: Total Reward = 2049.38\n","Episode 343: Total Reward = 2216.36\n","Episode 344: Total Reward = 2434.92\n","Episode 345: Total Reward = 2057.42\n","Episode 346: Total Reward = 2422.23\n","Episode 347: Total Reward = 2040.87\n","Episode 348: Total Reward = 2314.22\n","Episode 349: Total Reward = 2028.06\n","Episode 350: Total Reward = 2373.47\n","Episode 351: Total Reward = 2338.68\n","Episode 352: Total Reward = 2009.45\n","Episode 353: Total Reward = 2256.43\n","Episode 354: Total Reward = 2189.95\n","Episode 355: Total Reward = 2024.93\n","Episode 356: Total Reward = 2008.48\n","Episode 357: Total Reward = 2208.63\n","Episode 358: Total Reward = 2415.55\n","Episode 359: Total Reward = 2045.48\n","Episode 360: Total Reward = 2269.65\n","Episode 361: Total Reward = 2109.16\n","Episode 362: Total Reward = 2155.67\n","Episode 363: Total Reward = 2418.77\n","Episode 364: Total Reward = 2014.52\n","Episode 365: Total Reward = 2086.96\n","\n","Evaluation Results:\n","Average Reward over 365 episodes: 2221.26\n","Seasonal Performance:\n","  Summer: 0.00\n","  Winter: 2221.26\n","  Spring/Autumn: 0.00\n"]},{"output_type":"execute_result","data":{"text/plain":["(np.float64(2221.2560383576215),\n"," {1: np.float64(0.0), 2: np.float64(2221.2560383576215), 3: np.float64(0.0)})"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["#PPO Snapshot Assemble"],"metadata":{"id":"I3El2xerliXl"}},{"cell_type":"markdown","source":["Lets create plosts based on eta"],"metadata":{"id":"Kyhp68wFs-tf"}},{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","import torch\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from stable_baselines3 import A2C\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","import matplotlib.pyplot as plt\n","\n","# ---- Hyperparameters ---- #\n","TOTAL_TIMESTEPS = 10000\n","SNAPSHOT_CYCLES = 5\n","CYCLE_LENGTH = TOTAL_TIMESTEPS // SNAPSHOT_CYCLES\n","ETA_MIN_VALUES = [1e-3]\n","\n","env = DummyVecEnv([lambda: ElectricityMarketEnv()])\n","\n","def train_and_evaluate(seed):\n","    model = PPO('MlpPolicy', env, verbose=0, seed=seed)\n","    optimizer = model.policy.optimizer\n","\n","    snapshots = []\n","\n","    timesteps = 0\n","    while timesteps < TOTAL_TIMESTEPS:\n","        model.learn(total_timesteps=CYCLE_LENGTH, reset_num_timesteps=False)\n","        timesteps += CYCLE_LENGTH\n","\n","        snapshot = model.policy.state_dict()\n","        snapshots.append(snapshot)\n","        print(f\"Snapshot {len(snapshots)} saved at timestep {timesteps}\")\n","\n","    def ensemble_predict(observation):\n","        with torch.no_grad():\n","            actions = []\n","            values = []\n","            log_probs = []\n","            obs_tensor = torch.tensor(observation, dtype=torch.float32)\n","\n","            for snapshot in snapshots:\n","                model.policy.load_state_dict(snapshot)\n","                action, value, log_prob = model.policy.forward(obs_tensor)\n","\n","                actions.append(action.numpy())\n","                values.append(value.numpy())\n","                log_probs.append(log_prob.numpy())\n","\n","            avg_action = np.mean(actions, axis=0)\n","\n","            avg_value = np.mean(values)\n","            avg_log_prob = np.mean(log_probs)\n","\n","            return avg_action.flatten()\n","\n","    eval_env = ElectricityMarketEnv()\n","    obs, _ = eval_env.reset()\n","    total_rewards = []\n","\n","    for episode in range(50):\n","        done = False\n","        episode_reward = 0\n","        obs, _ = eval_env.reset()\n","\n","        while not done:\n","            action = ensemble_predict(obs.reshape(1, -1))\n","            obs, reward, done, truncated, _ = eval_env.step(action)\n","            episode_reward += reward\n","            if done or truncated:\n","                break\n","\n","        total_rewards.append(episode_reward)\n","\n","    avg_reward = np.mean(total_rewards)\n","    return avg_reward\n","\n","avg_rewards = []\n","for seed in SEEDS:\n","      avg_reward = train_and_evaluate(seed)\n","\n","      avg_rewards.append(avg_reward)\n","      print(seed)\n","\n","avg_rewards"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"KCtpryS_tBga","executionInfo":{"status":"error","timestamp":1740692753537,"user_tz":-120,"elapsed":437521,"user":{"displayName":"Alina Sudakov","userId":"08447656899639407846"}},"outputId":"5b5ae786-7f4f-49bd-ee3a-2cc3a79d3bc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Snapshot 1 saved at timestep 2000\n","Snapshot 2 saved at timestep 4000\n","Snapshot 3 saved at timestep 6000\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-c47c18f1b9ba>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mavg_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSEEDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m       \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m       \u001b[0mavg_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-c47c18f1b9ba>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtimesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mTOTAL_TIMESTEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Train for one cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCYCLE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mtimesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mCYCLE_LENGTH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 311\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dump_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                 \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# Normalize advantage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mevaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_features_extractor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mlatent_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_vf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mpi_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvf_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/torch_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0mall\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0mare\u001b[0m \u001b[0mshared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthen\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mlatent_policy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlatent_value\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \"\"\"\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/torch_layers.py\u001b[0m in \u001b[0;36mforward_actor\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}